import time
import uuid
import re
from typing import List, Dict, Any, AsyncGenerator
from openai import OpenAI
from .models import (
    ChatCompletionRequest,
    ChatCompletionResponse,
    ChatCompletionStreamResponse,
    Choice,
    StreamChoice,
    Delta,
    Usage,
    Message,
    Model,
    ModelListResponse
)
from .config import config
import json
import logging

logger = logging.getLogger(__name__)


class HuggingFaceConverter:
    """Hugging Face APIè½¬æ¢å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è½¬æ¢å™¨"""
        self.config = config
    
    def get_client(self, api_key: str = None):
        """è·å–OpenAIå®¢æˆ·ç«¯ï¼Œæ”¯æŒåŠ¨æ€API Key"""
        # ä¼˜å…ˆä½¿ç”¨å®¢æˆ·ç«¯ä¼ é€’çš„API Key
        if api_key:
            effective_api_key = api_key
            logger.info(f"ğŸ”‘ Using client API key: {api_key[:15]}...{api_key[-4:] if len(api_key) > 19 else ''}")
        elif self.config.hf_token:
            effective_api_key = self.config.hf_token
            logger.info(f"ğŸ”‘ Using server default API key: {effective_api_key[:15]}...{effective_api_key[-4:] if len(effective_api_key) > 19 else ''}")
        else:
            # æ²¡æœ‰ä»»ä½•API Keyæ—¶ï¼Œä½¿ç”¨å ä½ç¬¦ï¼ˆå®¢æˆ·ç«¯å¿…é¡»æä¾›æœ‰æ•ˆçš„keyï¼‰
            effective_api_key = "client-api-key-required"
            logger.warning("ğŸ”‘ No API key available - client must provide valid HF token")
        
        return OpenAI(
            base_url=self.config.hf_base_url,
            api_key=effective_api_key,
        )
    
    def convert_messages_to_hf_format(self, messages: List[Message]) -> List[Dict[str, str]]:
        """å°†OpenAIæ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸ºHugging Faceæ ¼å¼"""
        hf_messages = []
        for msg in messages:
            hf_messages.append({
                "role": msg.role.value,
                "content": msg.content
            })
        return hf_messages
    
    def generate_response_id(self) -> str:
        """ç”Ÿæˆå“åº”ID"""
        return f"chatcmpl-{uuid.uuid4().hex[:29]}"
    
    def get_current_timestamp(self) -> int:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        return int(time.time())
    
    def estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—tokenæ•°é‡ï¼ˆç®€å•å®ç°ï¼‰"""
        if text is None:
            return 0
        # ç®€å•çš„tokenä¼°ç®—ï¼Œå®é™…åº”è¯¥ä½¿ç”¨tokenizer
        return len(text.split()) + len(text) // 4
    
    def parse_thinking_content(self, content: str) -> tuple[str, str]:
        """è§£æthinkingå†…å®¹ï¼Œåˆ†ç¦»æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆå›ç­”"""
        if content is None:
            return "", ""
        
        # DeepSeekæ¨¡å‹ä½¿ç”¨çš„æ ¼å¼ï¼šthinkingå†…å®¹ç›´æ¥ä»¥</think>ç»“æŸï¼Œæ²¡æœ‰å¼€å§‹æ ‡ç­¾
        if '</think>' in content:
            # æ‰¾åˆ°</think>æ ‡ç­¾çš„ä½ç½®
            end_pos = content.find('</think>')
            thinking_content = content[:end_pos].strip()
            final_content = content[end_pos + 8:].strip()  # 8æ˜¯'</think>'çš„é•¿åº¦
            return thinking_content, final_content
        
        # å¦‚æœæ²¡æœ‰thinkingæ ‡ç­¾ï¼Œè¿”å›åŸå†…å®¹ä½œä¸ºæœ€ç»ˆå›ç­”
        return "", content
    
    async def create_chat_completion(
        self, 
        request: ChatCompletionRequest,
        api_key: str = None
    ) -> ChatCompletionResponse:
        """åˆ›å»ºèŠå¤©å®Œæˆï¼ˆéæµå¼ï¼‰"""
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            hf_messages = self.convert_messages_to_hf_format(request.messages)
            
            # ä½¿ç”¨åŠ¨æ€API Keyè·å–å®¢æˆ·ç«¯
            client = self.get_client(api_key)
            
            # è°ƒç”¨Hugging Face API
            response = client.chat.completions.create(
                model=request.model,
                messages=hf_messages,
                temperature=request.temperature,
                top_p=request.top_p,
                max_tokens=request.max_tokens,
                stop=request.stop,
                stream=False
            )
            
            # è½¬æ¢å“åº”æ ¼å¼
            return self._convert_hf_response_to_openai(response, request.model, request)
            
        except Exception as e:
            logger.error(f"Error in create_chat_completion: {str(e)}")
            raise
    
    async def create_chat_completion_stream(
        self, 
        request: ChatCompletionRequest,
        api_key: str = None
    ) -> AsyncGenerator[str, None]:
        """åˆ›å»ºæµå¼èŠå¤©å®Œæˆ"""
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            hf_messages = self.convert_messages_to_hf_format(request.messages)
            
            # ä½¿ç”¨åŠ¨æ€API Keyè·å–å®¢æˆ·ç«¯
            client = self.get_client(api_key)
            
            # è°ƒç”¨Hugging Face APIï¼ˆæµå¼ï¼‰
            stream = client.chat.completions.create(
                model=request.model,
                messages=hf_messages,
                temperature=request.temperature,
                top_p=request.top_p,
                max_tokens=request.max_tokens,
                stop=request.stop,
                stream=True
            )
            
            response_id = self.generate_response_id()
            created = self.get_current_timestamp()
            
            # ç”¨äºç´¯ç§¯å†…å®¹çš„å˜é‡ï¼ˆä»…ç”¨äºè°ƒè¯•ï¼‰
            accumulated_content = ""
            
            # å‘é€æµå¼å“åº”
            for chunk in stream:
                # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹ - æ·»åŠ Noneå€¼æ£€æŸ¥
                if chunk.choices and chunk.choices[0].delta.content is not None:
                    content = chunk.choices[0].delta.content
                    accumulated_content += content
                    
                    # ç›´æ¥é€ä¼ åŸå§‹å†…å®¹ï¼Œä¸åšä»»ä½•ç‰¹æ®Šå¤„ç†
                    # è¿™ç¡®ä¿äº†ä¸å®˜æ–¹APIå®Œå…¨ä¸€è‡´çš„è¾“å‡ºæ ¼å¼
                    stream_response = ChatCompletionStreamResponse(
                        id=response_id,
                        created=created,
                        model=request.model,
                        choices=[
                            StreamChoice(
                                index=0,
                                delta=Delta(content=content),
                                finish_reason=None
                            )
                        ]
                    )
                    yield f"data: {stream_response.model_dump_json()}\n\n"
                
                # æ£€æŸ¥æ˜¯å¦ç»“æŸ - æ”¹è¿›ç»“æŸæ£€æµ‹é€»è¾‘
                if chunk.choices and chunk.choices[0].finish_reason is not None:
                    # å‘é€ç»“æŸæ ‡è®° - ç¡®ä¿åŒ…å«finish_reasonçš„æœ€ç»ˆchunk
                    final_response = ChatCompletionStreamResponse(
                        id=response_id,
                        created=created,
                        model=request.model,
                        choices=[
                            StreamChoice(
                                index=0,
                                delta=Delta(),
                                finish_reason=chunk.choices[0].finish_reason
                            )
                        ]
                    )
                    yield f"data: {final_response.model_dump_json()}\n\n"
                    # ç«‹å³å‘é€[DONE]æ ‡è®°
                    yield "data: [DONE]\n\n"
                    return  # ä½¿ç”¨returnè€Œä¸æ˜¯breakç¡®ä¿å‡½æ•°å®Œå…¨ç»“æŸ
            
            # å¦‚æœå¾ªç¯æ­£å¸¸ç»“æŸä½†æ²¡æœ‰æ”¶åˆ°finish_reasonï¼Œä¹Ÿè¦å‘é€[DONE]
            # è¿™ç§æƒ…å†µå¯èƒ½å‘ç”Ÿåœ¨æŸäº›æ¨¡å‹æˆ–ç½‘ç»œé—®é¢˜æ—¶
            logger.warning("Stream ended without finish_reason, sending [DONE] anyway")
            yield "data: [DONE]\n\n"
            
        except Exception as e:
            logger.error(f"Error in create_chat_completion_stream: {str(e)}")
            # å‘é€é”™è¯¯ä¿¡æ¯
            error_response = {
                "error": {
                    "message": str(e),
                    "type": "internal_error",
                    "code": "internal_error"
                }
            }
            yield f"data: {json.dumps(error_response)}\n\n"
    
    def _convert_hf_response_to_openai(self, hf_response, model: str, request: ChatCompletionRequest) -> ChatCompletionResponse:
        """å°†Hugging Faceå“åº”è½¬æ¢ä¸ºOpenAIæ ¼å¼"""
        choice = hf_response.choices[0]
        
        # è§£æthinkingå†…å®¹
        raw_content = choice.message.content or ""
        thinking_content, final_content = self.parse_thinking_content(raw_content)
        
        # ä¼°ç®—tokenä½¿ç”¨é‡
        prompt_tokens = sum(self.estimate_tokens(msg.content) for msg in request.messages)
        completion_tokens = self.estimate_tokens(raw_content)
        
        return ChatCompletionResponse(
            id=self.generate_response_id(),
            created=self.get_current_timestamp(),
            model=model,
            choices=[
                Choice(
                    index=choice.index,
                    message=Message(
                        role=choice.message.role,
                        content=final_content,
                        reasoning=thinking_content if thinking_content else None
                    ),
                    finish_reason=choice.finish_reason
                )
            ],
            usage=Usage(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=prompt_tokens + completion_tokens
            )
        )
    
    async def get_models(self, api_key: str = None) -> ModelListResponse:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            # ä½¿ç”¨åŠ¨æ€API Keyè·å–å®¢æˆ·ç«¯
            client = self.get_client(api_key)
            
            # è°ƒç”¨Hugging Face APIè·å–æ¨¡å‹åˆ—è¡¨
            models_response = client.models.list()
            
            models = []
            for model in models_response.data:
                models.append(Model(
                    id=model.id,
                    created=getattr(model, 'created', self.get_current_timestamp()),
                    owned_by=getattr(model, 'owned_by', 'huggingface')
                ))
            
            return ModelListResponse(data=models)
            
        except Exception as e:
            logger.error(f"Error in get_models: {str(e)}")
            # è¿”å›é»˜è®¤æ¨¡å‹åˆ—è¡¨
            return ModelListResponse(
                data=[
                    Model(
                        id=config.default_model,
                        created=self.get_current_timestamp(),
                        owned_by="huggingface"
                    )
                ]
            )