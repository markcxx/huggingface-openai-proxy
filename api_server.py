from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from contextlib import asynccontextmanager
import logging
import uvicorn
import time
from typing import Union

from src.models import (
    ChatCompletionRequest,
    ChatCompletionResponse,
    ModelListResponse,
    ErrorResponse
)
from src.converter import HuggingFaceConverter
from src.config import config

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO if not config.debug else logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å…¨å±€è½¬æ¢å™¨å®ä¾‹
converter = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global converter
    
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    logger.info("Initializing Hugging Face API Proxy Server...")
    converter = HuggingFaceConverter()
    logger.info(f"Server starting on {config.host}:{config.port}")
    logger.info(f"Using Hugging Face base URL: {config.hf_base_url}")
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†
    logger.info("Shutting down server...")


# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="Hugging Face OpenAI API Proxy",
    description="A proxy server that converts OpenAI API requests to Hugging Face API calls",
    version="1.0.0",
    lifespan=lifespan
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=config.cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ç§»é™¤log_requestsä¸­é—´ä»¶ä»¥è§£å†³Verceléƒ¨ç½²é—®é¢˜
# è¯¥ä¸­é—´ä»¶åœ¨Vercel Serverlessç¯å¢ƒä¸­ä¼šå¯¼è‡´ASGIå¤„ç†å¼‚å¸¸
# @app.middleware("http")
# async def log_requests(request: Request, call_next):
#     """è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶"""
#     start_time = time.time()
#     
#     # è®°å½•è¯·æ±‚
#     logger.info(f"{request.method} {request.url.path} - Client: {request.client.host}")
#     
#     response = await call_next(request)
#     
#     # è®°å½•å“åº”æ—¶é—´
#     process_time = time.time() - start_time
#     logger.info(f"Request completed in {process_time:.2f}s - Status: {response.status_code}")
#     
#     return response


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "Hugging Face OpenAI API Proxy",
        "version": "1.0.0",
        "endpoints": {
            "chat_completions": "/v1/chat/completions",
            "models": "/v1/models"
        }
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "timestamp": converter.get_current_timestamp()}


@app.post("/v1/chat/completions")
async def create_chat_completion(request: ChatCompletionRequest, http_request: Request):
    """åˆ›å»ºèŠå¤©å®Œæˆ"""
    global converter
    
    # ç¡®ä¿converterå·²åˆå§‹åŒ–ï¼ˆVercelç¯å¢ƒfallbackï¼‰
    if converter is None:
        logger.warning("Converter not initialized, initializing now...")
        converter = HuggingFaceConverter()
    
    # ä»è¯·æ±‚å¤´è·å–å®¢æˆ·ç«¯çš„API Key
    auth_header = http_request.headers.get("Authorization")
    client_api_key = None
    if auth_header and auth_header.startswith("Bearer "):
        client_api_key = auth_header[7:]  # ç§»é™¤"Bearer "å‰ç¼€
        logger.info(f"ğŸ”‘ POST /v1/chat/completions - ä½¿ç”¨å®¢æˆ·ç«¯API Key: {client_api_key}")
    else:
        if config.hf_token:
            logger.warning(f"ğŸ”‘ POST /v1/chat/completions - æœªæ‰¾åˆ°å®¢æˆ·ç«¯API Keyï¼Œä½¿ç”¨æœåŠ¡ç«¯é»˜è®¤Key: {config.hf_token}")
        else:
            logger.warning(f"ğŸ”‘ POST /v1/chat/completions - æœªæ‰¾åˆ°å®¢æˆ·ç«¯API Keyï¼Œä¸”æ— æœåŠ¡ç«¯é»˜è®¤Keyï¼Œå®¢æˆ·ç«¯å¿…é¡»æä¾›æœ‰æ•ˆçš„HF Token")
        logger.info(f"ğŸ”‘ POST /v1/chat/completions - Authorizationå¤´å†…å®¹: {auth_header or 'None'}")
    
    try:
        logger.info(f"Chat completion request - Model: {request.model}, Stream: {request.stream}")
        
        if request.stream:
            # æµå¼å“åº” - ä½¿ç”¨åŒæ­¥ç”Ÿæˆå™¨ä»¥é¿å…Vercelç¯å¢ƒä¸‹çš„é—®é¢˜
            def generate():
                for chunk in converter.create_chat_completion_stream(request, client_api_key):
                    yield chunk
            
            return StreamingResponse(
                generate(),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "X-Content-Type-Options": "nosniff"
                }
            )
        else:
            # éæµå¼å“åº”
            response = await converter.create_chat_completion(request, client_api_key)
            logger.info(f"Chat completion successful - Response ID: {response.id}")
            return response
            
    except Exception as e:
        logger.error(f"Error in chat completion: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": {
                    "message": str(e),
                    "type": "internal_error",
                    "code": "internal_error"
                }
            }
        )


@app.get("/v1/models", response_model=ModelListResponse)
async def list_models(http_request: Request):
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    global converter
    
    # ç¡®ä¿converterå·²åˆå§‹åŒ–ï¼ˆVercelç¯å¢ƒfallbackï¼‰
    if converter is None:
        logger.warning("Converter not initialized, initializing now...")
        converter = HuggingFaceConverter()
    
    # ä»è¯·æ±‚å¤´è·å–å®¢æˆ·ç«¯çš„API Key
    auth_header = http_request.headers.get("Authorization")
    client_api_key = None
    if auth_header and auth_header.startswith("Bearer "):
        client_api_key = auth_header[7:]  # ç§»é™¤"Bearer "å‰ç¼€
        logger.info(f"ğŸ”‘ GET /v1/models - ä½¿ç”¨å®¢æˆ·ç«¯API Key: {client_api_key}")
    else:
        if config.hf_token:
            logger.warning(f"ğŸ”‘ GET /v1/models - æœªæ‰¾åˆ°å®¢æˆ·ç«¯API Keyï¼Œä½¿ç”¨æœåŠ¡ç«¯é»˜è®¤Key: {config.hf_token}")
        else:
            logger.warning(f"ğŸ”‘ GET /v1/models - æœªæ‰¾åˆ°å®¢æˆ·ç«¯API Keyï¼Œä¸”æ— æœåŠ¡ç«¯é»˜è®¤Keyï¼Œå®¢æˆ·ç«¯å¿…é¡»æä¾›æœ‰æ•ˆçš„HF Token")
    
    try:
        logger.info("Models list request")
        models = await converter.get_models(client_api_key)
        logger.info(f"Returned {len(models.data)} models")
        return models
        
    except Exception as e:
        logger.error(f"Error getting models: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": {
                    "message": str(e),
                    "type": "internal_error",
                    "code": "internal_error"
                }
            }
        )


@app.exception_handler(404)
async def not_found_handler(request: Request, exc):
    """404é”™è¯¯å¤„ç†"""
    return JSONResponse(
        status_code=404,
        content={
            "error": {
                "message": f"Path {request.url.path} not found",
                "type": "not_found",
                "code": "not_found"
            }
        }
    )


@app.exception_handler(422)
async def validation_exception_handler(request: Request, exc):
    """è¯·æ±‚éªŒè¯é”™è¯¯å¤„ç†"""
    return JSONResponse(
        status_code=422,
        content={
            "error": {
                "message": "Invalid request format",
                "type": "invalid_request_error",
                "code": "invalid_request_error",
                "details": str(exc)
            }
        }
    )


if __name__ == "__main__":
    import time
    
    # å¯åŠ¨æœåŠ¡å™¨
    uvicorn.run(
        "api_server:app",
        host=config.host,
        port=config.port,
        reload=config.debug,
        log_level="info" if not config.debug else "debug"
    )