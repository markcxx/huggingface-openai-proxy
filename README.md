# Hugging Face OpenAI API ä»£ç†æœåŠ¡

è¿™æ˜¯ä¸€ä¸ªåŸºäº FastAPI çš„ä»£ç†æœåŠ¡ï¼Œå°† OpenAI æ ¼å¼çš„ API è¯·æ±‚è½¬æ¢ä¸º Hugging Face API è°ƒç”¨ï¼Œè®©æ‚¨å¯ä»¥æ— ç¼ä½¿ç”¨ Hugging Face çš„æ¨¡å‹æœåŠ¡ã€‚

## ğŸš€ åŠŸèƒ½ç‰¹æ€§

- âœ… **OpenAI å…¼å®¹ API**ï¼šå®Œå…¨å…¼å®¹ OpenAI API æ ¼å¼
- âœ… **æµå¼å“åº”æ”¯æŒ**ï¼šæ”¯æŒ Server-Sent Events (SSE) æµå¼è¾“å‡º
- âœ… **å¤šæ¨¡å‹æ”¯æŒ**ï¼šæ”¯æŒ Hugging Face ä¸Šçš„å„ç§æ¨¡å‹
- âœ… **CORS æ”¯æŒ**ï¼šæ”¯æŒè·¨åŸŸè®¿é—®
- âœ… **é”™è¯¯å¤„ç†**ï¼šå®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
- âœ… **é«˜æ€§èƒ½**ï¼šåŸºäº FastAPI å’Œå¼‚æ­¥å¤„ç†

## ğŸ“‹ API ç«¯ç‚¹

- `POST /v1/chat/completions` - èŠå¤©å®Œæˆï¼ˆæ”¯æŒæµå¼å’Œéæµå¼ï¼‰
- `GET /v1/models` - è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
- `GET /health` - å¥åº·æ£€æŸ¥
- `GET /` - æœåŠ¡ä¿¡æ¯

## ğŸ› ï¸ å®‰è£…å’Œé…ç½®

### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ Condaï¼ˆæ¨èï¼‰

1. **åˆ›å»º conda è™šæ‹Ÿç¯å¢ƒ**ï¼š
```bash
conda env create -f environment.yml
conda activate huggingface
```

### æ–¹æ³•äºŒï¼šä½¿ç”¨ pip

1. **åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ**ï¼š
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```

2. **å®‰è£…ä¾èµ–**ï¼š
```bash
pip install -r requirements.txt
```

### é…ç½®ç¯å¢ƒå˜é‡

åˆ›å»º `.env` æ–‡ä»¶å¹¶é…ç½®ä»¥ä¸‹å˜é‡ï¼š

```env
# å¿…éœ€é…ç½®
HF_TOKEN=your_huggingface_token_here

# å¯é€‰é…ç½®
HF_BASE_URL=https://router.huggingface.co/v1
HOST=0.0.0.0
PORT=8000
DEBUG=false
API_PREFIX=/v1
CORS_ORIGINS=*
DEFAULT_MODEL=openai/gpt-oss-120b:fireworks-ai
REQUEST_TIMEOUT=300
```

**è·å– Hugging Face Token**ï¼š
1. è®¿é—® [Hugging Face Settings](https://huggingface.co/settings/tokens)
2. åˆ›å»ºä¸€ä¸ªæ–°çš„ Access Token
3. å°† token æ·»åŠ åˆ° `.env` æ–‡ä»¶ä¸­

## ğŸš€ å¯åŠ¨æœåŠ¡

### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ä¸»å¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰
```bash
python main.py
```

### æ–¹æ³•äºŒï¼šç›´æ¥ä½¿ç”¨uvicorn
```bash
uvicorn api_server:app --host 0.0.0.0 --port 8000
```

### æ–¹æ³•ä¸‰ï¼šå®‰è£…åä½¿ç”¨å‘½ä»¤è¡Œ
```bash
# å®‰è£…é¡¹ç›®
pip install -e .

# ä½¿ç”¨å‘½ä»¤è¡Œå¯åŠ¨
hf-openai-proxy
```

### ä½¿ç”¨ Dockerï¼ˆå¯é€‰ï¼‰
```bash
# æ„å»ºé•œåƒ
docker build -t hf-openai-proxy .

# è¿è¡Œå®¹å™¨
docker run -p 8000:8000 --env-file .env hf-openai-proxy
```

## ğŸ“– ä½¿ç”¨ç¤ºä¾‹

### 1. éæµå¼è¯·æ±‚

```python
import requests

url = "http://localhost:8000/v1/chat/completions"
headers = {"Content-Type": "application/json"}
data = {
    "model": "openai/gpt-oss-120b:fireworks-ai",
    "messages": [
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
    ],
    "temperature": 0.7,
    "max_tokens": 1000
}

response = requests.post(url, json=data, headers=headers)
print(response.json())
```

### 2. æµå¼è¯·æ±‚

```python
import requests
import json

url = "http://localhost:8000/v1/chat/completions"
headers = {"Content-Type": "application/json"}
data = {
    "model": "openai/gpt-oss-120b:fireworks-ai",
    "messages": [
        {"role": "user", "content": "è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"}
    ],
    "stream": True,
    "temperature": 0.7
}

response = requests.post(url, json=data, headers=headers, stream=True)

for line in response.iter_lines():
    if line:
        line = line.decode('utf-8')
        if line.startswith('data: '):
            data_str = line[6:]  # ç§»é™¤ 'data: ' å‰ç¼€
            if data_str.strip() == '[DONE]':
                break
            try:
                data = json.loads(data_str)
                if 'choices' in data and data['choices']:
                    content = data['choices'][0]['delta'].get('content', '')
                    if content:
                        print(content, end='', flush=True)
            except json.JSONDecodeError:
                continue
```

### 3. ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯

```python
from openai import OpenAI

# é…ç½®å®¢æˆ·ç«¯æŒ‡å‘æœ¬åœ°ä»£ç†æœåŠ¡
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy-key"  # ä»£ç†æœåŠ¡ä¸éœ€è¦çœŸå®çš„ OpenAI key
)

# éæµå¼è°ƒç”¨
response = client.chat.completions.create(
    model="openai/gpt-oss-120b:fireworks-ai",
    messages=[
        {"role": "user", "content": "ä½ å¥½ï¼Œä¸–ç•Œï¼"}
    ]
)
print(response.choices[0].message.content)

# æµå¼è°ƒç”¨
stream = client.chat.completions.create(
    model="openai/gpt-oss-120b:fireworks-ai",
    messages=[
        {"role": "user", "content": "è¯·è¯¦ç»†ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"}
    ],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

### 4. è·å–æ¨¡å‹åˆ—è¡¨

```python
import requests

response = requests.get("http://localhost:8000/v1/models")
models = response.json()
print(f"å¯ç”¨æ¨¡å‹æ•°é‡: {len(models['data'])}")
for model in models['data']:
    print(f"- {model['id']}")
```

## ğŸ”§ é…ç½®è¯´æ˜

| ç¯å¢ƒå˜é‡ | é»˜è®¤å€¼ | è¯´æ˜ |
|---------|--------|------|
| `HF_TOKEN` | æ—  | Hugging Face API Tokenï¼ˆå¿…éœ€ï¼‰ |
| `HF_BASE_URL` | `https://router.huggingface.co/v1` | Hugging Face API åŸºç¡€ URL |
| `HOST` | `0.0.0.0` | æœåŠ¡å™¨ç›‘å¬åœ°å€ |
| `PORT` | `8000` | æœåŠ¡å™¨ç«¯å£ |
| `DEBUG` | `false` | æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼ |
| `CORS_ORIGINS` | `*` | å…è®¸çš„è·¨åŸŸæ¥æº |
| `DEFAULT_MODEL` | `openai/gpt-oss-120b:fireworks-ai` | é»˜è®¤æ¨¡å‹ |
| `REQUEST_TIMEOUT` | `300` | è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ |

## ğŸ“ é¡¹ç›®ç»“æ„

```
Huggingface-api/
â”œâ”€â”€ src/                   # æ ¸å¿ƒæºä»£ç åŒ…
â”‚   â”œâ”€â”€ __init__.py        # åŒ…åˆå§‹åŒ–æ–‡ä»¶
â”‚   â”œâ”€â”€ models.py          # Pydantic æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ converter.py       # è¯·æ±‚/å“åº”è½¬æ¢å™¨
â”‚   â””â”€â”€ config.py          # é…ç½®ç®¡ç†
â”œâ”€â”€ api_server.py          # FastAPI åº”ç”¨å®šä¹‰
â”œâ”€â”€ main.py                # ä¸»å¯åŠ¨è„šæœ¬
â”œâ”€â”€ setup.py               # åŒ…å®‰è£…é…ç½®
â”œâ”€â”€ requirements.txt       # Python ä¾èµ–
â”œâ”€â”€ environment.yml        # Conda ç¯å¢ƒé…ç½®
â”œâ”€â”€ .env                   # ç¯å¢ƒå˜é‡ï¼ˆéœ€è¦åˆ›å»ºï¼‰
â”œâ”€â”€ .env.example           # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â”œâ”€â”€ .gitignore             # Git å¿½ç•¥æ–‡ä»¶
â””â”€â”€ README.md              # è¯´æ˜æ–‡æ¡£
```

## ğŸ§ª æµ‹è¯•

è¿è¡Œæµ‹è¯•è„šæœ¬ï¼š
```bash
python test_client.py
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **HF_TOKEN é”™è¯¯**
   - ç¡®ä¿åœ¨ `.env` æ–‡ä»¶ä¸­æ­£ç¡®è®¾ç½®äº† `HF_TOKEN`
   - éªŒè¯ token æ˜¯å¦æœ‰æ•ˆä¸”æœ‰è¶³å¤Ÿæƒé™

2. **æ¨¡å‹ä¸å¯ç”¨**
   - æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®
   - ç¡®è®¤æ‚¨çš„ Hugging Face è´¦æˆ·å¯ä»¥è®¿é—®è¯¥æ¨¡å‹

3. **è¿æ¥è¶…æ—¶**
   - æ£€æŸ¥ç½‘ç»œè¿æ¥
   - å¢åŠ  `REQUEST_TIMEOUT` å€¼

4. **CORS é”™è¯¯**
   - åœ¨ `.env` ä¸­æ­£ç¡®é…ç½® `CORS_ORIGINS`

### æ—¥å¿—æŸ¥çœ‹

æœåŠ¡å™¨ä¼šè¾“å‡ºè¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
- è¯·æ±‚ä¿¡æ¯
- å“åº”æ—¶é—´
- é”™è¯¯è¯¦æƒ…

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“ æ”¯æŒ

å¦‚æœæ‚¨é‡åˆ°é—®é¢˜ï¼Œè¯·ï¼š
1. æŸ¥çœ‹æ—¥å¿—è¾“å‡º
2. æ£€æŸ¥é…ç½®æ˜¯å¦æ­£ç¡®
3. æäº¤ Issue æè¿°é—®é¢˜